---
title: "Permutation Tests"
author: "Livio Finos"
date: "University of Padova"
output: 
  pdf_document:
    keep_tex: yes
    number_sections: yes
    toc: yes
  html_document:
    toc: yes
    # toc_float: yes
    number_sections: yes
  beamer_presentation: default  
  ioslides_presentation:
    logo: figs/logoUnipd.jpg
---

# Introduction

## Introduction

- Well established nonparametric approach to **inference**: Fisher, 1935; Pitman, 1937; Pitman, 1938.  
- (In general) it requires less assumptions about the data generating process than the parametric counterpart.   
- Very good inferential properties, typically:  
    * exactness (i.e. exact control of the type I error)
    * asymptotically optimality and convergence to the parametric counterpart when it does exist.

<!-- ---- -->

- Fisher exact test is a prototypical example, but
-  the general approach has restricted applicability without the support of a computer. 

<!-- This is the case of very small datasets where all possible rearrangement of the data can be enumerated. In all other cases, only special cases can be treated ’by hand’. The most well-known example is the comparison of two independent samples with a binomial response, that is, the  -->

## Renewed interest toward permutation testing

- A milestone: Westfall and Young (1993). Resampling-Based Multiple Testing: 
Examples and Methods for p-value Adjustment. Wiley.
- Many actives areas of research adopt these methods in their daily statistical analysis (e.g. genetics and neuroscience: Nichols and Holmes (2002); Pantazis et al. (2009); Winkler et al. (2014)).  
- Permutation approach:  
    * Ideal for **randomized experimental design**  
    * deals with very complex
models,  without formal definition of the data generating process.

## The package `flip`

It is on CRAN and on github (https://github.com/livioivil/flip)

To install the github version type (in R):

```{r,eval=FALSE,echo=TRUE}
library(devtools)
install_github('livioivil/flip')
```
 


<!-- ---- -->

**Before we start**

```{r}
#clean the memory
rm (list=ls ())


# We customize the output of our graphs a little bit
par.old=par ()
par (cex.main=1.5, lwd=2, col="darkgrey", pch=20, cex=3)
# par (par.old)
palette (c ("#FF0000", "#00A08A", "#FFCC00", "#445577", "#45abff"))

# customize the output of knitr
knitr :: opts_chunk$set (fig.align="center")#, fig.width=6, fig.height=6)
```

## The Age vs Reaction Time Dataset

The reaction time of these subjects was tested by having them grab a meter stick after it was released by the tester. The number of centimeters that the meter stick dropped before being caught is a direct measure of the person’s response time.

The values of `Age` are in years. The `Gender` is coded as `F` for female and  `M` for male.
The values of `Reaction.Time` are in centimeters.

(data are fictitious)

To read the data

```{r}
data(reaction,package = "flip")
# or download it from: https://github.com/livioivil/flip/tree/master/data
# str (reaction)
```

<!-- --- -->

We plot the data

```{r,fig.height=4,fig.width=4}
plot(x=reaction$Age,y=reaction$Reaction.Time,pch=20,col=2,cex=2)
```

## Measuring the dependence between two variables

we define:

- $X=Age$  
- $Y=Reaction.Time$

We review some famous index to measure the (linear) dependence among two variables

### Covariance and Variance

**Covariance** between $X$ and $Y$:

$\sigma_{xy}=\frac{\sum_{i=1} ^ n (x_i- \bar{x}) (y_i- \bar{y} )}{n}$

- values between $- \infty$ and $\infty$  
- $\sigma_{xy} \approx 0$: there is no dependency between $X$ and $Y$  
- $\sigma_{xy} >> (<<) 0$: there is a strong positive (negative) dependency between $X$ and $Y$  

<!-- --- -->

**Variance** of $X$ 
<!-- (= covariance between $X$ and $X$): -->

$\sigma_{xx}=\sigma_{x} ^ 2= \frac{\sum_{i=1} ^ n (x_i- \bar{x}) ^ 2}{n}$

**Standard Deviation** of $X$:

$\sigma_{xx}=\sqrt{\sigma_{xx}}=\sigma_{x}$

<!-- **(Co)Variance** of $X$:   -->
<!-- $\sigma_{xy} ^ o=\frac{\sum_{i=1} ^ n (x_i- \bar{x}) (y_i- \bar{ y})}{n-1}$ -->


<!-- (similarly for variance and standard deviation) -->


### Correlation
With the Covariance it is difficult to understand when the relationship between $X$ and $Y$ is strong/weak.
We note that

$- \sigma_{x} \sigma_{y} \leq \sigma_{xy} \leq \sigma_{x} \sigma_{y}$
<!-- we divide each memeber by $\sigma_x \sigma_y$: -->
is quivalent to
$-1 \leq \frac{\sigma_{xy}}{\sigma_{x} \sigma_{y}} \leq 1$


**Correlation** between $X$ and $Y$:

$\rho_{xy}=\frac{\sigma{xy}}{\sigma_{x} \sigma_{y}} =
\frac{\sum_{i=1} ^ n (x_i- \bar{x}) (y_i- \bar{y})}{\sqrt{\sum_{i=1} ^ n (x_i- \bar{ x}) ^ 2} \sqrt{\sum_{i=1} ^ n (y_i- \bar{y}) ^ 2}}$

- values between $-1$ and $1$
- $\rho_{xy} \approx 0$: there is no dependency between $X$ and $Y$
- $\rho_{xy} \approx 1 (-1)$: there is a strong positive (negative) dependency between $X$ and $Y$


### Linear Trend, the least squares method
We describe the relationship between   
`Reaction.Time` and
`Age` with a straight line.

$E(Reaction.Time) \approx \beta_0 + \beta_1 Age$  
$E(Y)=\beta_0 + \beta_1X$


Let's draw a line 'in the middle' of the data.

<!-- ---  -->

The **least-squares estimator**

We look for the one that passes more 'in the middle', the one that minimizes the sum of the squares of the residues:

$\hat{\beta}_0$ and $\hat{\beta}_1$ such that  
$\sum_{i=1} ^ n (y_i - (\hat{\beta}_0 + \hat{\beta}_1x_i )) ^ 2$
is minimum.

<!-- --- -->

```{r,echo=FALSE}
model=lm(Reaction.Time~Age,data=reaction)
```

Estimates:  

- Angular coefficient: $\hat{\beta}_1=\frac{\sigma_{xy}}{\sigma_{xx}}=\rho_{xy}\frac{\sigma_{y}}{\sigma_{x}}=\frac{\sum_{i=1}^n(x_i- \bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2}=$ `r coefficients(model)[2]`  
- Intercept: $\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}=$ `r coefficients(model)[1]`
- Response (estimated $y$): $\hat{y}_i=\hat{\beta}_0 + \hat{\beta}_1x_i$
- Residuals (from the estimated response):
$y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)=y_i- \hat{y}_i$


and therefore the least squares are the sum of the squared residuals:
$\sum_{i=1} ^ n (y_i- \hat{\beta}_0 + \hat{\beta}_1x_i) ^ 2=\sum_{i=1} ^ n (y_i- \hat{y}_i ) ^ 2$

<!-- --- -->

A graphical representation:

```{r}
model=lm(Reaction.Time~Age,data=reaction)
coefficients(model)
```

<!-- --- -->

```{r}
plot(reaction$Age,reaction$Reaction.Time,pch=20,col=2,cex=1)
coeff=round(coefficients(model),1)
title(paste("Y=",coeff[1],"+",coeff[2],"*X"))
abline(model,col=1)
``` 

<!-- ## Interpretation of the coefficients -->

<!-- - $\beta_0$ indicates the value of $y$ when $x=0$ (where the line intersects the ordinate axis).  -->
<!-- - $\beta_1$ indicates how much $y$ grows as a unit of $x$ grows  -->
<!--   - If $\beta_1=0$ there is no relation between $x$ and $y$.$Y$is constant (horizontal ), knowing$x$does not change the estimate of $y$ -->
<!--   - If $\beta_1> (<) 0$the relation between $x$ and $y$is positive (negative). When $X$passes from$x$ a$x + 1$the estimate of $Y$goes from$\hat{y}$to$\hat{y} + \hat{\beta}_1$ -->


# Permutation approach to Hypothesis Testing

### Some remarks

Let's note that all the measures above does not make any assumptions on the random process that generate them.

Let now assume that $Y$ - and possibly $X$ - is generated by a random variable.

Further minimal assumptions will be specified later.

<!-- ---  -->

The question:
**Is there a relationship between $Y$ and $X$**?

We estimated 
$\hat{\beta}_1=$ `r coefficients(model)[2]`

But the **true value** $\beta_1$ is really different from 0 (i.e. no relationship)?  
Otherwise, is the difference from 0 due to the random sampling?


- **Null Hypothesis** $H_0: \ \beta_1=0$ (the **true** $\beta_1$, not its estimate $\hat{\beta}_1$!).
There is no relationship between $X$ and $Y$.

- **Alternative Hypothesis **$H_1: \ \beta_1 >0$
The relationship is positive.

Other possible specifications of $H_1: \ \beta_1< 0$ and, more commonly,  $H_1: \ \beta_1 \neq 0$.



<!-- ## Distribution of $\hat{\beta}_1$ (Statistics Test) if $H_0$ is true -->

<!-- Suppose *the null hypothesis $H_0$ is true*. -->

<!-- What are the possible values of $\beta_1$ conditional to the observed values $X$ and $Y$? -->

<!-- On the data we calculated: -->
<!-- ```{r} -->
<!-- model=lm (Reaction.Time ~ Age, data=reaction) -->
<!-- # Estimated beta_1: -->
<!-- coefficients (model) [ "Age"] -->
<!-- ``` -->



## Permutation tests - in a nutshell
As a toy example, let use a sub-set of the data:
```{r, echo=FALSE,fig.height=4,fig.width=4}
(reactionREd=reaction[2:4,])

plot (reactionREd$Age, reactionREd$Reaction.Time, pch=20, col=1)
abline (lm (Reaction.Time ~ Age, data=reactionREd))
```

<!-- --- -->

- *If $H_0$* is true: there is no linear relationship between $X$ and $Y$
- Therefore, the trend observed on the data is due to chance.
- Any other match of $x_i$ and $y_i$ was equally likely to occur
- I can generate the datasets of other hypothetical experiments by exchanging the order of the observations in $Y$.
- How many equally likely datasets could I get with$X$ and $Y$observed?
$3 * 2 * 1=3!=6$ possible datasets.

Remark: Here we only assume that $y$ is a random variable. The only assumption here is the exchangeability of the observations: the joint density $f(y_1,\ldots,y_n)$ does not change when the ordering of $y_1,\ldots,y_n$ is changed.

### All potential datasets

```{r, echo=FALSE}
make_perm_plot<-function(yperm=sample(reaction$Age),reactionPerm=reaction){
  reactionPerm$Reaction.Time=yperm
  model=lm(Reaction.Time~Age,data=reactionPerm)
  plot(reactionPerm$Age,reactionPerm$Reaction.Time,pch=20,col=3,cex=2,xlab="Age",ylab="Reaction.Time")
  coeff=round(coefficients(model),2)
  title(paste("Y=",coeff[1],"+",coeff[2],"*X"))
  abline(model,col=2)
}

Y=cbind (reactionREd[,3],reactionREd[c(1,3,2),3],reactionREd[c(2,1,3),3],reactionREd[3:1,3],reactionREd[c(2,3,1),3],reactionREd[c(3,1,2),3])
X=reactionREd$Age

# Y=cbind (c (2,1,4), c (1,2,4), c (1,4,2),
# c (4,1,2), c (4,2,1), c (2,4,1)) * 10
# X=1: 3
# cbind (X=X, Y)
par (mfrow=c (2,3))
for (i in 1: 6){
make_perm_plot(Y[,i],reactionPerm = reactionREd)
}
par (mfrow=c (1,1))
```

<!-- --- -->

#### In our data set

We apply the same principle to the complete dataset...

How many permutations of the vector $y_1,\ldots,y_n$ are possible? $n!=10!=3628800$.

big, perhaps not too big ... but what happen with, for example, $n=20$? We got $20!=2.432902e+18$. This is too big, definitely! 

We calculate a smaller (but sufficiently large) $B$ of random permutations. 

here some example

<!-- --- -->

**`Age` vs a permutations of `Reaction.Time`**

```{r, results='asis', echo=FALSE}

par(mfrow=c(2,3))
for(i in 1:6)
  make_perm_plot()
par(mfrow=c(1,1))
```

<!-- --- -->

We repeat `r (B=10000)` times and we look at the histogram of the $\hat{\beta}_1$

```{r}
# beta_1 estimated on the observed data:
beta1=coefficients(lm(Reaction.Time~Age,data=reaction))[2]

# function that permutes the y values and calculates the coeff beta_1
my.beta.perm <- function(Y,X){
  model=lm(sample(Y)~X)
  coefficients(model)[2]
}

#replicate it B-1 times
beta.perm= replicate(B,my.beta.perm(reaction$Reaction.Time, reaction$Age ))
```

<!-- ---- -->

```{r,echo=FALSE}

#the observed dataset is one of the possible permutations
beta.perm=c(beta1,beta.perm)
# str(beta.perm)
hist(beta.perm,50)
points(beta1,0,lwd=3,col=1)
```

### How likely WAS $\hat{\beta}_1 ^{obs}$?
(before the experiment!)

How likely was it to get a $\leq \hat{\beta}_1 ^{obs}$ value among the many possible values of $\hat{\beta}_1 ^{*b}$ (obtained by permuting data)?


Remarks:

- $\hat{\beta}_1 ^{* b}< \hat{\beta}_1 ^{obs}$ (closer to 0): less evidence against $H_1$ than $\hat{\beta}_1 ^{obs}$
- $\hat{\beta}_1 ^{* b} \geq \hat{\beta}_1 ^{obs}$: equal or more evidence towards $H_1$ than $\hat{\beta}_1 ^{obs}$


### Calculation of the p-value

Over B=`r B` permutations we got `r sum (beta.perm <= beta1)` times a 
$\hat{\beta}_1 ^{* b} \leq \hat{\beta}_1 ^{obs}$.

The p-value (significance) is
$p=\frac{\# (\hat{\beta}_1 ^{* b} \geq \hat{\beta}_1 ^{obs})}{B} =$ `r (p=sum(beta.perm>= beta1)/B)`

($\hat{\beta}_1 ^{obs}$ counts as a random permutation)
<!-- ```{r} -->
<!-- hst=hist(beta.perm,plot=FALSE,50) -->
<!-- hist(beta.perm,col=(hst$breaks[-1]<=beta1)+2, probability=TRUE ,50) -->
<!-- points(beta1,0,lwd=3,col=1) -->
<!-- ``` -->


 
### Interpretation

The probability of $p=P (\hat{\beta}_1 ^ * \geq \hat{\beta}_1=$ `r round (coefficients (model), 3)[2]` $| H_0)$ is equal to $p =$ `r p`, i.e. very small.  
So, it was unlikely to get a value like this **IF $H_0$ is true**.

Neyman-Pearson's approach has made common the use of a significance threshold for example $\alpha=.05$ (or $=. 01$).
When $p \leq \alpha$ rejects the hypothesis that there is no relationship between X and Y ($H_0$). If so, we are inclined to think that $H_1$ is true (there is a positive relationship).

- Type I error: False Positive  
the true hypo is $H_0$ (null correlation), BUT we accept $H_1$ (correlation is positive)
- Type II error: False Negative  
the true hypo is $H_1$ (positive correlation), BUT we do not reject $H_0$ (null correlation)


## To sum up

**p-value**: proportion of experiments providing equal or more evidence against $H_0$ with respect to observed data.

To compute it, we need the 
**Orbit $\mathcal{O}$** and a
**Test statistic** ($T:\ \mathbb{R}^n\to\mathbb{R}$) quantifies the evidence against $H_0$

- higher values provide more evidence against $H_0$
- compute a test statistic for each element of the Orbit $\mathcal{O}$, this induces an ordering on $\mathcal{O}$.


In our example: $T=\hat\beta_1=\hat\sigma_{xy}/\hat\sigma_{yy}$ is the (estimated) slope.  
Higher the slope, higher the evidence for $H_1$. 

<!-- ---- -->

**Type I error control**

We want to guarantee not to get false relationships (a few false positives), better to be conservative. To make this, we want to bound the probability to make a false discovery:

$P (p-value \leq \alpha | H_0) \leq \alpha$

We built a machinery that in the long run (many replicates of the experiment) finds false correlations with probability $\alpha$ (e.g. $0.05=5\%$).


### We make it in flip

```{r}
library(flip)

(res=flip(Reaction.Time~Age,data=reaction,tail=1))
## compare also with
# flip(Reaction.Time~Age,data=reaction,tail=1,statTest = "cor")
# flip(Reaction.Time~Age,data=reaction,tail=1,statTest = "coeff")
```


<!-- ---- -->

```{r}
plot(res)
```

<!-- ---- -->

**Type I error control**

We want to guarantee not to get false relationships (a few false positives), better to be conservative. To make this, we want to bound the probability to make a false discovery:

$P (p-value \leq \alpha | H_0) \leq \alpha$

We built a machinery that in the long run (many replicates of the experiment) finds false correlations with probability $\alpha$ (e.g. $0.05=5\%$).


### Composite alternatives (bilateral)

The hypothesis $H_1: \ \beta_1 >0$ (the relation is positive) must be justified with a priori knowledge.


More frequently, the Alternative hypothesis is appropriate:
$H_1: \ \beta_1 \neq 0$
(there is a relationship, I do not assume the direction)

I consider anomalous coefficients estimated as very small but also very large ('far from 0').
The p-value is 

$p=\frac{\#(|\hat{\beta}_1^{*b} | \geq|\hat{\beta}_1^{obs}|)}{B}=$ `r (sum(beta.perm>=beta1)+sum(beta.perm<=-beta1))/B`

(remark: the observed test stat is included among the permuted one)

<!-- ---- -->

In `flip`:
```{r}
library(flip)
(res=flip(Reaction.Time~Age,data=reaction,tail=0,perms=10000))
```


<!-- ---- -->

```{r}
plot(res)
```

## A more formal approach

(see also Pesarin, 2001; Hemerik \& Goeman, 2017)

Let $Y$ be data taking values in a sample space $\mathcal{Y}$. Let $\Pi$ be a finite set of transformations
$\pi : \mathcal{Y} \rightarrow \mathcal{Y}$, such that $\Pi$ is a **group** with respect to the operation of composition of
transformations, that is:   

- it contains identity, 
- every element has an inverse in the group,
- closure: if $\pi_1,\pi_2\in\Pi$: $\pi_1\circ\pi_2\in\Pi$

(e.g. $\Pi$ set of all possible permutations)

**Null Hypothesis**  
$H_0:$ $Y\in \Omega_0$

**Randomization Hypothesis**  Under the null hypothesis,
the distribution of $Y$ is invariant under the transformations in $\Pi$; that is, for
every $\pi$ in $\Pi$, $\pi Y$ and $Y$ have the same distribution whenever $Y$ has distribution P in $\Omega_0$.

(See also Lehmann, E. L., & Romano, J. P. (2006). Testing statistical hypotheses. Springer Science & Business Media.)

<!-- ---- -->

**Test statistic** $T(Y):\ \mathbb{R}^n\to\mathbb{R}$

<!-- ---- -->

$T^{(k)}(Y)$ is the $\lceil(1-\alpha)|\Pi|\ \rceil$-th sorted value of $T(\pi Y)$

Define the test:
\begin{equation}
\phi(Y) = 
\begin{cases} 
1 & \text{if } T(Y)\geq T^{(k)}(Y) \\
0 & \text{if } otherwise
\end{cases}
\end{equation}


**Theorem**: Under $H_0$, $E_P(\phi(Y))=\alpha$, that is $P(T(Y) \geq T^{(k)}) \leq \alpha$.

**Proof**

By construction, $\sum_{\pi\in \Pi} \phi(\pi Y)=|\Pi|\alpha$.
Therefore $|\Pi|\alpha= E_P(\sum_{\pi\in\Pi}\phi(\pi Y))= \sum_{\pi\in\Pi}E_P(\phi(\pi Y))$

Next, by the null hypothesis:
$E_P(\phi(Y))=E_P(\phi(\pi Y))$,  
so that
$|\Pi|\alpha= \sum_{\pi\in\Pi}E_P(\phi(Y))=|\Pi|E_P(\phi(Y))$
gives   
$E_P(\phi(Y))=\alpha$


(See also Lehmann, E. L., & Romano, J. P. (2006). Testing statistical hypotheses. Springer Science & Business Media.)


**More about permutation testing**

<!-- The null hypothesis $H_0$ implies that the joint distribution -->
<!-- of the test statistics $T(\pi Y), \pi \in \Pi$, is invariant under all transformations in $\Pi$ of $Y$. -->
<!-- That is, writing $\Pi = \{\pi_1,\ldots, \pi_{|\Pi|}\}$, under $H_0$: -->

<!-- $$T(\pi_1 Y), \ldots, T(\pi_{|\Pi|}Y) \overset{d}{=} T(\pi_1g Y), \ldots, T(\pi_{|\Pi|}gY)$$ -->
<!-- for all $g \in \Pi$. -->

<!-- Note that it holds when for all $\pi \in \Pi$: $Y \overset{d}{=}\pi Y$. -->


<!-- ---- -->

<!-- $\mathbf{y}=(y_1,y_2,\ldots,y_n)$ the vector of observed data -->

**Orbit** of $\mathcal{O}$: 
$$\mathcal{O}=\{\pi Y : \pi \in \Pi\} \subseteq \mathcal{Y}.$$

(losely) the set of all samples having the same likelihood under $H_0$.  
$$\mathcal{O}=\{\pi \mathbf{y}:\ f(\pi \mathbf{y})=f(\mathbf{y}) \}$$
($|\mathcal{O}|$ number of elements of $\mathcal{O}$)

If we assume exchangeability of observations, then:
$$\mathcal{O}=\{\textrm{all permutations of the observed data }\mathbf{y}\} = \{\mathbf{y}^*:\pi^*\circ\mathbf{y}\}$$

<!-- ---- -->

<!-- ---- -->

**Remark about assumption of exchangeability**: This means that, Under the Null Hypothesis, observations within subject are assumed to be exchangeable: e.g. $f(y_1,y_2)=f(y_2,y_1)$. 


This assumption is always true as long as observations:  

- are **identically distributed**,   
- have the **same dependence**, e.g. the same correlation.   


Parametric $t$-test and linear models assumes independence (more stringent than 'same dependence'), and normality of the errors, i.e. more severe assumptions than permutation approach.

When normality is not met, the parametric approach only provides asymptotic control of the tye I error, while permutation approach provides exactness.



**An Intuition about the proof** for an alternative proof of the control of the type I error

$$f(\mathbf{y}|\mathcal{O})=\frac{f(\mathbf{y}\cap\mathcal{O})}{f(\mathcal{O})}=
\frac{f(\mathbf{y})}{f(\mathcal{O})}=
\frac{f(\mathbf{y})}{f(\cup_{y\in\mathcal{O}}y)}=\frac{1}{|\mathcal{O}|}\ \forall\ \mathbf{y}\in \mathcal{O}$$
i.e. each permutation is equally likely in the Orbit $\mathcal{O}$.

(due to group structure)
\[
\begin{aligned}
&E(\phi(Y)|\mathbf{y}\in\mathcal{O}, H_0)=\\
&P(T(\mathbf{y})\geq T^{(k)} | \mathbf{y}\in\mathcal{O}, H_0)=\\
&=\int_{T^{(k)}}^{+\infty} f(T(\mathbf{y}))dT(\mathbf{y})=\\
&=\sum_{\mathbf{y}\in\mathcal{O}} I(T(\mathbf{y})\geq T^{}/|\mathcal{O}|\leq \alpha
\ \ \ \ \forall\mathcal{O}
\end{aligned}
\]

And now
$E(\phi(\mathbf{y}))=\int_P E(\phi(\mathbf{y})|\mathbf{y}\in\mathcal{O}, H_0) d\mathbf{y}$




### Properties (see Pesarin, 2001)

The theorem above proves that the permutation tests have **exact control of the type I error**, i.e. $P(p-value\leq \alpha|H_0)=\alpha$ assuming $\alpha\in \{1/|\mathcal{O}|,2/|\mathcal{O}|,\ldots,1\}$ - don't forget that the orbit $\mathcal{O}$ is a finite set; if this is not the case, the test is (slightly) conservative.

Further properties:  

- The permutations tests are **Unbiased**: $P(p-value\leq \alpha|H_1)>\alpha$  
- The test is **Consistent**: $P(p-value\leq \alpha|H_1)\to 1$ when $n\to\infty$  
- The test converges to the parametric counterpart (when it exists)


## A comparison (and relationships) with parametric linear model

We can see that the histogram of the statistical tests (calculated on the permuted data) is well described by a **Gaussian **(normal) curve.

<!-- ---- -->

```{r}
hist(beta.perm,50,probability=TRUE,col=2)
curve(dnorm(x,mean(beta.perm),sd(beta.perm)),add=TRUE,col=1,lwd=3)
points(beta1,0,lwd=3,col=1)
```

### The (simple) linear parametric model
We assume that the observed values are distributed around true values
$\beta_0 + \beta_1 X$ according to a Gaussian law:

$Y=\textrm{linear part} + \textrm{normal error}$

$Y=\beta_0 + \beta_1 X + \varepsilon$

**Assumptions of the linear model **

- the $\boldsymbol{y_i=\beta_0 + \beta_1 x_i + \varepsilon_i}$
the relationship between X and Y is truly linear, less than the error term $\varepsilon_i$
<!-- -  the **observations ** are **independent** each others ( -->
<!-- knowing the value of the $y_i$observation does not help me to predict the value of $y_{i + 1}$) -->
- $\boldsymbol{\varepsilon_i \sim N (0, \sigma ^ 2), \ \forall i=1, \ldots, n}$ errors have normal distribution with zero mean and common variance (homoschedasticity: same variance).



### Hypothesis testing
If these assumptions are true,

$\hat{\beta_1} \sim N (\beta_1, \sigma ^ 2 / \sum (x_i- \bar{x}) ^ 2)$

We calculate the test statistic:

$t=\frac{\hat{\beta_1}}{std.dev\ \hat{\beta_1}}=\frac{\hat{\beta_1}}{\sqrt{\sum_{i=1} ^ n (y_i- \hat{y}_i) ^ 2 / \sum (x_i- \bar{x}) ^ 2 / (n-2)}}$

If $H_0: \beta_1=0$, $t \sim t (n-2)$ is true

On `reaction` data and $H_1: \beta_1 \neq 0$ (bilateral alternative)


```{r}
model=lm (Reaction.Time ~ Age, data=reaction)
summary (model)
```

Similar result, but much more assumptions!


### Assumptions of a permutation test

What model do we assume in a permutation test?

Under the null hypo:
$H_0:\ f(y)=f(y|x) \ \forall x$

Under the alternative hypo no assumptions. in order to have power we hope that:

$H_1:\ E(y|x)=\beta_0 + \beta_1x; \textrm{ with } \beta_1\neq 0 \textrm{ and for some } x$  
that is:  
$H_1: E(yx)\neq E(x)E(y)$

No other assumptions on the distribution of $f(y|x)$ (normality, nor finite moments)

## Permutationally equivalent tests

```{r}
set.seed(1)
(res_cor=flip(Reaction.Time~Age,data=reaction,statTest = "cor"))
set.seed(1)
(res_t=flip(Reaction.Time~Age,data=reaction,statTest = "t"))

plot(res_cor@permT,res_t@permT,pch=20,col=2)
```


### Conclusion

**The permutation tests**:

- Different from bootstrap methods. The former are extractions without reintegration, the latter with. The former have almost optimal properties and have (almost always) an exact control of the first type errors.
- They constitute a general approach and are applicable in many contexts. Very few assumptions.  
- some dedicated R packages: 
    * `coin` <http://cran.r-project.org/web/packages/coin/index.html>
    * `permuco` <https://cran.r-project.org/web/packages/permuco/index.html>
    * `flip` <http://cran.r-project.org/web/packages/flip/index.html> (the development version is on github <https://github.com/livioivil/flip>)
    * `flipscores` <http://cran.r-project.org/web/packages/flipscores/index.html> (the development version is on github <https://github.com/livioivil/flipscores>)
    * `multcomp` <https://cran.r-project.org/web/packages/multcomp/index.html>
    * `GFD` <https://cran.r-project.org/web/packages/GFD/index.html>
    
    

# Some special cases

## Rank-correlation

- $n$ observations from $y$, we are interested on $F(y|x)$
    - we don't need $y_1$ and $y_2$ do be continuous, we don't even need to have finite moments (usual minimal assumption).

- Hypotheses
    - $H_0: F(y|x)=F(y|x')\ \forall x,x'$  
    - $H_1: \exists \ x< x' : F(y|x)< F(y|x')$ or directional such as: $H_1: \exists x,x' F(y_1)\neq F(y_2)$

- Test Statistic: rank-correlation
     

```{r}
(res=flip(Reaction.Time~Age,data=reaction,perms = 10000,statTest  = "rank"))

# to see the rank correlation use the workaround:
(res=flip(rank(reaction$Reaction.Time)~rank(reaction$Age),perms = 10000,statTest  = "cor"))


(cor.test(reaction$Reaction.Time,reaction$Age,method="spe"))

```

<!-- ---- -->

```{r}
plot(res)
```

## The Two-independent-sample problem

* Two samples: 
    - $n_1$ observations from $y_1$  
    - $n_2$ observations from $y_2$  
    - we don't need $y_1$ and $y_2$ do be continuous, we don't even neeD to have second (nor higher order) finite moments, which is the usual minimal assumption.

* Hypotheses
    - $H_0: F(y_1)=F(y_2)$  
    - $H_1: F(y_1)\neq F(y_2)$  
    (or directional such as: $H_1: F(y_1)<F(y_2)$)

<!-- ---- -->

* Test Statistic: 
     - Standardized mean difference (t-statistic)  
     - Estimated slope coefficient (label of groups as dummy predictor)  
     - other test statistic such as the (non standardized) mean difference are permutationally equivalent 

```{r}
data("seeds")
seeds=na.omit(seeds)

(res=flip(y~grp,data=seeds,perms = 10000))

(summary(lm(y~grp,data=seeds)))

```
<!-- ---- -->

```{r}
plot(res)
```

<!-- ---- -->

### Rank test

Can we use rank-based statistics?

Yes, equivalent to rank-tests, we just rely on exact distribution instead of asymptotic one (and we have no limitations with ties).


```{r}
(res=flip(y~grp,data=seeds,statTest = "rank",perms=10000))

(wilcox.test(y~grp,data=seeds))
```

## Chi square and other cathegorical methods


```{r}
data("seeds")
seeds$Germinated=!is.na(seeds$x)
seeds$Germinated=factor(seeds$Germinated)
seeds$grp=factor(seeds$grp)


table(seeds$grp,seeds$Germinated)

```

<!-- ---- -->


```{r}
chisq.test(seeds$grp,seeds$Germinated)

(res=flip(Germinated~grp,data=seeds,statTest = "Chisq",perms=10000))
```
<!-- --- -->

```{r,fig.height=5,fig.width=5}
plot(res)
```

<!-- --- -->

... and the Fisher test:

```{r,fig.height=5,fig.width=5}
fisher.test(seeds$grp,seeds$Germinated)$p.value

(flip(Germinated~grp,data=seeds,perms=10000))
```

## ANOVA (C-sample)

e.g. 3 groups of `Age`: young [18-35), middle age [35-60), old [60-100)

* C samples: 
    - $n_i$ observations from $y_i$ ($i=1,\ldots,C$)
    - we don't need $y_i$ do be continuous, we don't even need to have finite moments (usual minimal assumption)

* Hypotheses
    - $H_0: F(y_i)=F(y_j)\ \forall (i,j)$  
    - $H_1: \exists (i,j): \ F(y_i)\neq F(y_j)$ 

<!-- ---- -->


* Test Statistic: 
     - F-statistic  
     - $R^2$  
     - other test statistic such as the (non standardized) mean difference are permutationally equivalent 
     - Rank-based is also possible


```{r}
reaction$AgeCateg=cut(reaction$Age,c(18,35,65,100),right = FALSE)
  
(res=flip(Reaction.Time~AgeCateg,data=reaction,perms = 10000,statTest  = "ANOVA"))

summary(lm(Reaction.Time~AgeCateg,data=reaction))

```

<!-- ---- -->

### Stochastic Ordering
* Same assumptions of ANOVA  
* Hypotheses  
    - same null hypo $H_0: F(y_i)=F(y_j)\ \forall (i,j)$  
    - BUT $H_1: \exists (i,j): \ F(y_i)< F(y_j)$ (or $>$)

(more details on NPC later)

```{r}
(res=flip(Reaction.Time~AgeCateg,data=reaction,perms = 10000,tail=1))
npc(res)
```


## Stratified permutations (discrete nuisances)

What if we want to test $x=$ `Age` also using $z=$ `Gender` as nuisance in the `reaction` data set?

Under the null hypothesis:
 $f(y|x,z)=f(y|x',z)=f(y|z) \ \forall (x,x')$

Therefore, even under the $H_0$, it holds $f(y_i)=f(y_j)$ ONLY IF $z_i=z_j$ (obs $i$ and $j$ have the same gender).

Can e permute same as in the previous cases? NO. We permute the observations only within the strata defined by $z$.

**Remark**:  
- we don't assume linear effect of the nuisance,   
- we also allow heteroscedastic errors among strata.

(Test statistic remains the same)

<!-- ```{r} -->
<!-- library(flip) -->
<!-- (res=flip(Reaction.Time~Age,data=reaction,perms=10000)) -->
<!-- ``` -->

<!-- ---- -->

<!-- ## Stratified permutations (nuisances) -->

```{r}
(res=flip(Reaction.Time~Age,Strata=~Gender,data=reaction,perms=10000))
```

<!-- ---- -->

<!-- Stratified permutations (nuisances) -->
Alternative model (more about NPC later):

```{r}
(res=flip(Reaction.Time~Age*Gender,Strata=~Gender,data=reaction,perms=10000))
npc(res)
```


# Multivariate Testing


## Seeds data
```{r,message=FALSE}
# install.packages("flip")
library(flip)
```

omit the `NA`s:

```{r}
data(seeds,package = "flip")
seeds=na.omit(seeds)
seeds
```

Use a permutation methods to test if there is any difference between the two groups in `grp` on the two variables `x` and `y`:

- perform the two tests for the two variables
- Combine the two p-values using the Fisher Combining Function to test the global hypothesis
- Use a closed testing procedure to adjust the 2 p-values.

## Joint distribution
```{r}
library(flip)
res=flip(.~grp,data=seeds)
hist(res)
plot(res)

# Global p-value
npc(res,"Fisher")

# adjusted p: Closed testing with Fisher combination
flip.adjust(res,"Fisher")
```

## Rejection regions

Ask for the multivariate distribution of the p-values: 
```{r}
res=flip(.~grp,data=seeds,flipReturn =list(permP=TRUE,permT=TRUE))
res.fisher=npc(res,"Fisher",flipReturn =list(permP=TRUE,permT=TRUE))
res.tippett=npc(res,"minP",flipReturn =list(permP=TRUE,permT=TRUE))
```

### Fisher Combining Function

We inspect the rejection regions of the two univariate tests and the one of Fisher combination.  
The intersection of each univariate test with the Fisher region defines the rejection region of a closed testing - i.e. adjusted for multiple testing.

```{r,echo=FALSE}
Falpha5=res.fisher@permT[which.min(abs(res.fisher@permP-.05))]
plot(res@permP[,1],res@permP[,2],col="#F2AD00",bg="#F98400",pch=21,main="Alpha .05",asp=1)

curve(exp(-Falpha5)/x,ylim=c(0,1),from=0.00001,to=1,add=TRUE,col="red",lwd=2)
abline(v=.05,col="green",lwd=2)
abline(h=.05,col="blue",lwd=2)
legend("topright",legend=c("H0(x,y) (Fisher)","H0(x)","H0(y)"),col=2:4,bty="n",lwd=2)
```


### Tippett (min-p) Combining Function
We inspect the rejection regions of the two univariate tests and the one of Fisher combination.  
The intersection of each univariate test with the Fisher region defines the rejection region of a closed testing - i.e. adjusted for multiple testing.
This fall to be the same rejection region given by Wesfall \& Young. Indeed, it is a closed testing with shortcut.


```{r,echo=FALSE}
Falpha5=-res.tippett@permT[which.min(abs(res.tippett@permP-.05))]
plot(res@permP[,1],res@permP[,2],col="#F2AD00",bg="#F98400",pch=21,main="Alpha .05",asp=1)

lines(c(Falpha5,Falpha5),c(Falpha5,1),col="red",lwd=2)
lines(c(Falpha5,1),c(Falpha5,Falpha5),col="red",lwd=2)
abline(v=.05,col="green",lwd=2)
abline(h=.05,col="blue",lwd=2)
legend("topright",legend=c("H0(x,y) (Tippett)","H0(x)","H0(y)"),col=2:4,bty="n",lwd=2)
```

# FWER control via Permutations tests

## Permutation Bonferroni

Bonferroni is conservative  

- **Bonferroni bound**  
    Reject for p-values at most $\alpha/m$  
- **By Boole's inequality**  
    Guaranteed: FWER $\leq \alpha$, but often FWER $<\alpha$  
- **Can we improve?**  
    Reject for p-values at most $\tilde\alpha > \alpha/m$, while keeping FWER control  
- **Yes we can**  
    By permutations

## Improved Bonferroni
- **Reduced $\alpha$**  
     Reject $H_i$ if $p_i \leq \tilde\alpha$
- **Control of FWER?**

\[
\begin{aligned}
\mathrm{FWER} &= \mathrm{P} \big(\textrm{$p_i \leq \tilde\alpha$ for at least one $i$ with $H_i$ true} \big) \\
    &= \mathrm{P} \Big( \bigcup_{i\in T} \{p_i \leq \tilde\alpha\} \Big) \\
    &= \mathrm{P} \Big( \min_{i \in T} p_i \leq \tilde\alpha \Big) \leq \alpha
\end{aligned}
\]

- **How can we determine the value of $\tilde \alpha$?**  
    Using permutations to find the distribution of the minimum p-value

## Multiple testing using permutations
**The single step min-P method**  

- Calculate the smallest p-value $m$ for the real data  
- Randomly permute the data  
- Calculate new p-values for all tests based on permuted data  
- Calculate the smallest p-value $m^\pi$ for permuted data  
- Repeat permutation many (say k=1000) times: $m^\pi_1, \ldots, m^\pi_k$  
- Calculate $\tilde\alpha$ as the $\alpha$-quantile of $m^\pi_1, \ldots, m^\pi_k$


**Multiple testing result**  
    Reject all hypotheses with (non-permuted) p-values at most $\tilde\alpha$


## Correlation structure of p-values

**Permutation**  
- Destroys correlation between covariates and response  
- Retains correlation among covariates

**Consequence**  
- P-values of correlated tests (i.e. data) remain correlated in permutations  
- Distribution of minimum p-value correctly takes correlations into account  

<!-- ---- -->

**When the gain relative to Bonferroni is the gain large?**  
- Negatively correlated p-values: typically no gain  
- Independent p-values: minimal gain  
- Positively correlated p-values: gain can be large

## Westfall \&\ Young: permutation Holm

*Westfall PH, Young SS (1993) Resampling-Based Multiple Testing: Examples and Methods for p-Value Adjustment. Wiley*

**Sequential permutation multiple testing**

- **Single step**  
    Single step min-P is permutation equivalent of Bonferroni  
- **What about Holm?**  
    Permutation equivalent of Holm's method: Westfall \&\ Young


<!-- ---- -->

**The min-P algorithm**  

* Start with all hypotheses
*  Repeat  
    - Do single step min-P to calculate $\tilde\alpha$  
    - Reject hypotheses with p-value $\leq \tilde\alpha$  
    - Remove rejected hypotheses  
* Until no new rejections occur



## Closed Testing

*R Marcus, E Peritz, KR Gabriel (1976). On closed testing procedures with special reference to ordered analysis of variance. Biometrika 63: 655-660.*

Test in each node: any multivariate permutation test


### Closure Set


```{r, echo=FALSE}
knitr::include_graphics("./figs/closed_set.PNG")
```

<!-- ```{r,engine='tikz',fig.ext='svg',fig.width=3,echo=FALSE} -->
<!-- \begin{center} -->
<!-- \begin{tikzpicture}[scale=2.5] -->
<!-- \draw[] -->
<!-- (0,4) node[draw, line width=1pt] {ABC} -->
<!-- (-1.5,2) node[draw, line width=1pt] {AB} -->
<!-- (0,2) node[draw, line width=1pt] {AC} -->
<!-- (1.5,2) node[draw, line width=1pt] {BC} -->
<!-- (-2,0) node[draw, line width=1pt] {A} -->
<!-- (0,0) node[draw, line width=1pt] {B} -->
<!-- (2,0) node[draw, line width=1pt] {C}; -->
<!-- \end{tikzpicture} -->
<!-- \end{center} -->
<!-- ``` -->

Adjusted $\tilde p_A=\max(p_A,p_{AB},p_{AC},p_{ABC})$


## Conclusion

**Accounting for dependencies**

Adjusted p-value become lower (i.e. more rejections)

**When?**  
- Negative correlation: generally no gain  
p-value Independents: little or no gain  
- Positive correlation: big gain, usually   
(NB: a test with bi-directional alternative and with negative correlation produce p-value positively correlated)

**Real data**  
The variables of real data sets are often correlated  
then permutations are (often) convenient

**How?** `R: library(flip); flip(); flip.adjust()`

<!-- ### Summary -->

<!-- **FamilyWise Error** -->

<!-- - Generalize the Type I error to the case of multiple hypotheses   -->
<!-- - Control the probability of at LEAST a false rejections   -->
<!-- - corrects the p-value (adjusted p-value always equal to or greater than the unadjusted p-value) -->


# A case study: Pharmacokinetic Study of Carbidopa 

Description:  
<http://webserv.jcu.edu/math//faculty/TShort/Bradstreet/part2/part2-table6.html>

As part of a pharmacokinetic study, 12 healthy male subjects were allocated randomly to a three period crossover design receiving one of three graded doses (25, 50, 100 mg) of Carbidopa q8h in each treatment period. A seven day washout period separated the treatment periods. The pharmacokinetic variables AUC, Cmax, and Tmax were calculated for each subject from plasma concentrations assayed from blood samples taken at 0, 0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, and 8 hours postdosing following the second dose of carbidopa on the sixth day of each treatment period.


dataset:  
<http://webserv.jcu.edu/math//faculty/TShort/Bradstreet/part2/Bradp2t6.txt>


Analyze the dataset without taking in account the Study Periods (which have been randomized in each subject, hence we can avoid to account for it in the analysis).

Research questions:

- Is there a dose response for AUC, Cmax, or Tmax? Overall?
- Can dose proportionality be established? (try to fit a linear model for each endpoint, then discuss the results)


## A solution


We answer to both first and second question with a single analysis: we perform a linear model (accounting for individual variability) on log transformed end-points.


```{r}
#Reading and make-up of the data

dati=read.table("http://webserv.jcu.edu/math//faculty/TShort/Bradstreet/part2/Bradp2t6.txt",skip = 1,header = TRUE)

dati=cbind(dati[,1],matrix(as.matrix(dati[,-1]),nrow(dati)*3,4))
colnames(dati)=c("Sub","Dose","AUC","Cmax","Tmax")

dati=as.data.frame(dati)
str(dati)

# transform all responses with log-transformed, 
# so that a linear relationship between time and end-point indicates proportionality 
dati[,3:5]=log(dati[,3:5])
```



```{r}
#Descriptives and plots:
summary(dati[,-1])
by(dati[,3:5],dati$Dose,summary)

par(mfrow=c(2,2))
plot(dati$Dose,dati$AUC,ylab="log(AUC)",xlab="Dose",main="Dose vs log(AUC)")

r=sapply(unique(dati$Sub),function(s){
  d=subset(dati,Sub==s)
  d=d[order(d$Dose),]
  lines(d$Dose,(d$AUC),col=s,lwd=2)})


plot(dati$Dose,dati$Cmax,ylab="log(Cmax)",xlab="Dose",main="Dose vs log(Cmax)")
r=sapply(unique(dati$Sub),function(s){
  d=subset(dati,Sub==s)
  d=d[order(d$Dose),]
  lines(d$Dose,(d$Cmax),col=s,lwd=2)})


plot(dati$Dose,dati$Tmax,ylab="log(Tmax)",xlab="Dose",main="Dose vs log(Tmax)")
r=sapply(unique(dati$Sub),function(s){
  d=subset(dati,Sub==s)
  d=d[order(d$Dose),]
  lines(d$Dose,(d$Tmax),col=s,lwd=2)})

```


Now the analysis: A simple solution could be:

```{r}
library(flip)
res=flip(.~Dose,data=dati,Strata=~Sub,statTest = "coeff")
summary(res)
#here we ask for statTest = "coeff", i.e. estimated coefficient of a linear model
hist(res)
```


Multivariate:

- Overall

```{r}
res=flip.adjust(res)
npc(res,"Fisher")
```

There is an effect of `Dose`, overall.


- By end-points (closed testing with max-t combining function). 
Try also different methods (e.g. `method="Fisher"`) and compare the results of `method="minP"` with the one of `method="Holm"`. 

```{r}
res=flip.adjust(res,method="holm")
res=flip.adjust(res,method="Fisher")
summary(res)
```

`AUC` and `Cmax` show a significant effect after correction for multiplicity, while `Tmax` does not.



# (minimal) Bibliography

The Grounding Theory:  
- Pesarin (2001) Multivariate Permutation Tests: With Applications in Biostatistics by Fortunato, Wiley, New York

An alternative approach to the Permutation testing:  
- Hemerik J, Goeman J. Exact testing with random permutations. Test (Madr). 2018;27(4):811-825. doi: 10.1007/s11749-017-0571-1. Epub 2017 Nov 30. PMID: 30930620; PMCID: PMC6405018.  


A flexible approach to General Linear Model based on the sign-flip score test:  
- Hemerik, Goeman and Finos (2020) Robust testing in generalized linear models by sign flipping score contributions. Journal of the Royal Statistical Society Series B (Statistical Methodology) 82(3). DOI: 10.1111/rssb.12369  
Implemented in R package flipscores:  
<https://cran.r-project.org/web/packages/flipscores/index.html>  
better to use the github develop version:  
<https://github.com/livioivil/flipscores>


A nice review of the regression model within the permutation framework:   
- Anderson M. Winkler, Gerard R. Ridgway, Matthew A. Webster, Stephen M. Smith, Thomas E. Nichols (2014)
Permutation inference for the general linear model, NeuroImage, Volume 92, Pages 381-397, ISSN 1053-8119 <https://doi.org/10.1016/j.neuroimage.2014.01.060>
